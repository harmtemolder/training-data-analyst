{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Exercise, now with hyperparameter tuning via Cloud ML Engine \n",
    "\n",
    "Create a neural network that is capable of finding the volume of a cylinder given the radius of its base (r) and its height (h). Assume that the radius and height of the cylinder are both in the range 0.5 to 2.0. Unlike in the challenge exercise for b_estimator.ipynb, assume that your measurements of r, h and V are all rounded off to the nearest 0.1. Simulate the necessary training dataset. This time, you will need a lot more data to get a good predictor.\n",
    "<p>\n",
    "Now modify the \"noise\" so that instead of just rounding off the value, there is up to a 10% error (uniformly distributed) in the measurement followed by rounding off."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'cylinders'\n",
    "BUCKET = 'cylinders'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bash\n",
    "import os\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['TFVERSION'] = '1.8'  # Tensorflow version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate cylinders and upload to bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 8000 cylinders to cylinders_train.csv\n",
      "saved 1000 cylinders to cylinders_eval.csv\n",
      "saved 1000 cylinders to cylinders_test.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://cylinders_test.csv [Content-Type=text/csv]...\n",
      "/ [0 files][    0.0 B/ 12.0 KiB]                                                \r",
      "/ [1 files][ 12.0 KiB/ 12.0 KiB]                                                \r",
      "Copying file://cylinders_train.csv [Content-Type=text/csv]...\n",
      "/ [1 files][ 12.0 KiB/107.6 KiB]                                                \r",
      "/ [2 files][107.6 KiB/107.6 KiB]                                                \r",
      "Copying file://cylinders_eval.csv [Content-Type=text/csv]...\n",
      "/ [2 files][107.6 KiB/119.6 KiB]                                                \r",
      "/ [3 files][119.6 KiB/119.6 KiB]                                                \r\n",
      "Operation completed over 3 objects/119.6 KiB.                                    \n",
      "Copying file://generate_cylinders.py [Content-Type=text/x-python]...\n",
      "/ [0 files][    0.0 B/  2.8 KiB]                                                \r",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \r\n",
      "Operation completed over 1 objects/2.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Generate cylinders locally (used in local test) and upload to bucket (used in hyperparameter tuning job)\n",
    "python generate_cylinders.py --filename \"cylinders_train.csv\" --size 8000\n",
    "python generate_cylinders.py --filename \"cylinders_eval.csv\" --size 1000\n",
    "python generate_cylinders.py --filename \"cylinders_test.csv\" --size 1000\n",
    "gsutil cp \"cylinders_*.csv\" \"gs://$BUCKET/\"\n",
    "gsutil cp \"generate_cylinders.py\" \"gs://$BUCKET/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create command-line program\n",
    "\n",
    "In order to submit to Cloud ML Engine, we need to create a distributed training program. Let's convert our housing example to fit that paradigm, using the Estimators API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf cylinder_prediction_module\n",
    "mkdir cylinder_prediction_module\n",
    "mkdir cylinder_prediction_module/trainer\n",
    "touch cylinder_prediction_module/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cylinder_prediction_module/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cylinder_prediction_module/trainer/task.py\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "from . import model\n",
    "    \n",
    "if __name__ == '__main__' and 'get_ipython' not in dir():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type = float, \n",
    "        default = 0.01\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type = int, \n",
    "        default = 30\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--output_dir',\n",
    "        help = 'GCS location to write checkpoints and export models.',\n",
    "        required = True\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--job-dir',\n",
    "        help = 'this model ignores this field, but it is required by gcloud',\n",
    "        default = 'junk'\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "    arguments = args.__dict__\n",
    "\n",
    "    # Unused args provided by service\n",
    "    arguments.pop('job_dir', None)\n",
    "    arguments.pop('job-dir', None)\n",
    "\n",
    "    # Append trial_id to path if we are doing hptuning\n",
    "    # This code can be removed if you are not using hyperparameter tuning\n",
    "    arguments['output_dir'] = os.path.join(\n",
    "        arguments['output_dir'],\n",
    "        json.loads(\n",
    "            os.environ.get('TF_CONFIG', '{}')\n",
    "        ).get('task', {}).get('trial', '')\n",
    "    )\n",
    "\n",
    "    # Run the training\n",
    "    shutil.rmtree(arguments['output_dir'], ignore_errors=True) # start fresh each time\n",
    "\n",
    "    # Pass the command line arguments to our model's train_and_evaluate function\n",
    "    model.train_and_evaluate(arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cylinder_prediction_module/trainer/model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cylinder_prediction_module/trainer/model.py\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "# Read cylinder CSVs from bucket\n",
    "import generate_cylinders\n",
    "traindf = generate_cylinders.generate_cylinder_df(8000)\n",
    "evaldf = generate_cylinders.generate_cylinder_df(2000)\n",
    "\n",
    "# Train and eval input functions\n",
    "def train_input_fn(df, batch_size):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = traindf,\n",
    "        y = traindf['volume'],\n",
    "        num_epochs = None,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True)\n",
    "\n",
    "def eval_input_fn(df, batch_size):\n",
    "    return tf.estimator.inputs.pandas_input_fn(\n",
    "        x = evaldf,\n",
    "        y = evaldf['volume'],\n",
    "        num_epochs = 1,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False)\n",
    "\n",
    "# Define feature columns\n",
    "features = [\n",
    "    tf.feature_column.numeric_column(key='radius', dtype=tf.float64),\n",
    "    tf.feature_column.numeric_column(key='height', dtype=tf.float64)]\n",
    "\n",
    "def train_and_evaluate(args):\n",
    "    # Compute appropriate number of steps.\n",
    "    num_steps = (len(traindf) / args['batch_size']) / args['learning_rate']\n",
    "    # Thus, if learning_rate = 0.01, hundred epochs\n",
    "\n",
    "    # Create custom optimizer\n",
    "    myopt = tf.train.FtrlOptimizer(learning_rate=args['learning_rate'])\n",
    "\n",
    "    # Create rest of the estimator as usual\n",
    "    estimator = tf.estimator.LinearRegressor(\n",
    "        model_dir = args['output_dir'], \n",
    "        feature_columns = features, \n",
    "        optimizer = myopt)\n",
    "    \n",
    "    #Add rmse evaluation metric\n",
    "    def rmse(labels, predictions):\n",
    "        pred_values = tf.cast(predictions['predictions'], tf.float64)\n",
    "        return {'rmse': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, rmse)\n",
    "\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = train_input_fn(df = traindf, batch_size = args['batch_size']),\n",
    "        max_steps = num_steps)\n",
    "\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = eval_input_fn(df = evaldf, batch_size = len(evaldf)),\n",
    "        steps = None)\n",
    "\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring with TensorBoard\n",
    "Use \"refresh\" in Tensorboard during training to see progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>TensorBoard was started successfully with pid 7645. Click <a href=\"/_proxy/36259/\" target=\"_blank\">here</a> to access it.</p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7645"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "OUTDIR = './cylinders_trained'\n",
    "TensorBoard().start(OUTDIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model locally to see if everything works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py3env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "INFO:tensorflow:TF_CONFIG environment variable: {'cluster': {}, 'environment': 'cloud', 'job': {'args': ['--batch_size=30', '--learning_rate=0.02', '--output_dir=cylinders_trained', '--job-dir', 'cylinders_trained'], 'job_name': 'trainer.task'}, 'task': {}}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_global_id_in_cluster': 0, '_session_config': None, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_train_distribute': None, '_is_chief': True, '_evaluation_master': '', '_task_type': 'worker', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f54f5e7bda0>, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_master': '', '_service': None, '_task_id': 0, '_model_dir': 'cylinders_trained/', '_save_checkpoints_secs': 600}\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_global_id_in_cluster': 0, '_session_config': None, '_keep_checkpoint_every_n_hours': 10000, '_save_summary_steps': 100, '_train_distribute': None, '_task_id': 0, '_evaluation_master': '', '_task_type': 'worker', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f550f842470>, '_keep_checkpoint_max': 5, '_log_step_count_steps': 100, '_master': '', '_service': None, '_is_chief': True, '_model_dir': 'cylinders_trained/', '_save_checkpoints_secs': 600}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into cylinders_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 2391.0999, step = 1\n",
      "INFO:tensorflow:global_step/sec: 524.186\n",
      "INFO:tensorflow:loss = 1314.7058, step = 101 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 621.821\n",
      "INFO:tensorflow:loss = 1090.8212, step = 201 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.989\n",
      "INFO:tensorflow:loss = 1749.7048, step = 301 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.729\n",
      "INFO:tensorflow:loss = 1773.6743, step = 401 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 619.044\n",
      "INFO:tensorflow:loss = 1333.9265, step = 501 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.854\n",
      "INFO:tensorflow:loss = 1177.2499, step = 601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.326\n",
      "INFO:tensorflow:loss = 1974.6978, step = 701 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 570.441\n",
      "INFO:tensorflow:loss = 1599.9996, step = 801 (0.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 578.581\n",
      "INFO:tensorflow:loss = 778.39075, step = 901 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 595.266\n",
      "INFO:tensorflow:loss = 1175.5535, step = 1001 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 596.026\n",
      "INFO:tensorflow:loss = 1346.4889, step = 1101 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 592.707\n",
      "INFO:tensorflow:loss = 849.74817, step = 1201 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.636\n",
      "INFO:tensorflow:loss = 528.89246, step = 1301 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 633.857\n",
      "INFO:tensorflow:loss = 908.2746, step = 1401 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.34\n",
      "INFO:tensorflow:loss = 915.603, step = 1501 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.595\n",
      "INFO:tensorflow:loss = 769.2428, step = 1601 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 621.585\n",
      "INFO:tensorflow:loss = 686.3517, step = 1701 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 593.427\n",
      "INFO:tensorflow:loss = 617.0587, step = 1801 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.187\n",
      "INFO:tensorflow:loss = 1045.6241, step = 1901 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.902\n",
      "INFO:tensorflow:loss = 678.2331, step = 2001 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.962\n",
      "INFO:tensorflow:loss = 719.88245, step = 2101 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.432\n",
      "INFO:tensorflow:loss = 668.1339, step = 2201 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.097\n",
      "INFO:tensorflow:loss = 736.32654, step = 2301 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.266\n",
      "INFO:tensorflow:loss = 507.1738, step = 2401 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 624\n",
      "INFO:tensorflow:loss = 284.40677, step = 2501 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 556.216\n",
      "INFO:tensorflow:loss = 933.8139, step = 2601 (0.180 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.87\n",
      "INFO:tensorflow:loss = 625.50287, step = 2701 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.534\n",
      "INFO:tensorflow:loss = 435.48254, step = 2801 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.249\n",
      "INFO:tensorflow:loss = 574.99, step = 2901 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.818\n",
      "INFO:tensorflow:loss = 383.24323, step = 3001 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 631.484\n",
      "INFO:tensorflow:loss = 846.54236, step = 3101 (0.158 sec)\n",
      "INFO:tensorflow:global_step/sec: 628.081\n",
      "INFO:tensorflow:loss = 934.76196, step = 3201 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.415\n",
      "INFO:tensorflow:loss = 459.29883, step = 3301 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.485\n",
      "INFO:tensorflow:loss = 722.008, step = 3401 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 629.602\n",
      "INFO:tensorflow:loss = 1212.4113, step = 3501 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.978\n",
      "INFO:tensorflow:loss = 583.1435, step = 3601 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.688\n",
      "INFO:tensorflow:loss = 480.42627, step = 3701 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 537.296\n",
      "INFO:tensorflow:loss = 443.39944, step = 3801 (0.186 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.986\n",
      "INFO:tensorflow:loss = 491.84262, step = 3901 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 619.686\n",
      "INFO:tensorflow:loss = 529.27673, step = 4001 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 630.001\n",
      "INFO:tensorflow:loss = 349.4013, step = 4101 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.617\n",
      "INFO:tensorflow:loss = 400.09625, step = 4201 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 619.891\n",
      "INFO:tensorflow:loss = 659.21594, step = 4301 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.851\n",
      "INFO:tensorflow:loss = 265.04584, step = 4401 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.257\n",
      "INFO:tensorflow:loss = 757.30023, step = 4501 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.655\n",
      "INFO:tensorflow:loss = 546.77997, step = 4601 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.566\n",
      "INFO:tensorflow:loss = 750.41077, step = 4701 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.67\n",
      "INFO:tensorflow:loss = 462.74567, step = 4801 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.364\n",
      "INFO:tensorflow:loss = 202.93008, step = 4901 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.459\n",
      "INFO:tensorflow:loss = 546.06616, step = 5001 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 609.318\n",
      "INFO:tensorflow:loss = 321.22723, step = 5101 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.609\n",
      "INFO:tensorflow:loss = 305.07443, step = 5201 (0.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 592.681\n",
      "INFO:tensorflow:loss = 570.50793, step = 5301 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.053\n",
      "INFO:tensorflow:loss = 643.4836, step = 5401 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 573.789\n",
      "INFO:tensorflow:loss = 738.5802, step = 5501 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 599.548\n",
      "INFO:tensorflow:loss = 433.88718, step = 5601 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 496.711\n",
      "INFO:tensorflow:loss = 614.56995, step = 5701 (0.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 581.796\n",
      "INFO:tensorflow:loss = 594.6307, step = 5801 (0.172 sec)\n",
      "INFO:tensorflow:global_step/sec: 593.034\n",
      "INFO:tensorflow:loss = 529.124, step = 5901 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 589.694\n",
      "INFO:tensorflow:loss = 576.9701, step = 6001 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 593.28\n",
      "INFO:tensorflow:loss = 463.1267, step = 6101 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 610.44\n",
      "INFO:tensorflow:loss = 253.80212, step = 6201 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.248\n",
      "INFO:tensorflow:loss = 614.33997, step = 6301 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.671\n",
      "INFO:tensorflow:loss = 372.07898, step = 6401 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 612.499\n",
      "INFO:tensorflow:loss = 583.0504, step = 6501 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 590.895\n",
      "INFO:tensorflow:loss = 793.15344, step = 6601 (0.169 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.078\n",
      "INFO:tensorflow:loss = 339.30072, step = 6701 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 577.444\n",
      "INFO:tensorflow:loss = 424.2969, step = 6801 (0.173 sec)\n",
      "INFO:tensorflow:global_step/sec: 588.186\n",
      "INFO:tensorflow:loss = 517.2161, step = 6901 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.297\n",
      "INFO:tensorflow:loss = 629.40265, step = 7001 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 623.914\n",
      "INFO:tensorflow:loss = 665.16095, step = 7101 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 626.902\n",
      "INFO:tensorflow:loss = 515.99036, step = 7201 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.148\n",
      "INFO:tensorflow:loss = 457.7513, step = 7301 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.271\n",
      "INFO:tensorflow:loss = 456.61392, step = 7401 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.645\n",
      "INFO:tensorflow:loss = 526.004, step = 7501 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 622.978\n",
      "INFO:tensorflow:loss = 372.3091, step = 7601 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.286\n",
      "INFO:tensorflow:loss = 441.8975, step = 7701 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.35\n",
      "INFO:tensorflow:loss = 502.4395, step = 7801 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 624.796\n",
      "INFO:tensorflow:loss = 555.074, step = 7901 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 625.671\n",
      "INFO:tensorflow:loss = 364.9161, step = 8001 (0.160 sec)\n",
      "INFO:tensorflow:global_step/sec: 627.203\n",
      "INFO:tensorflow:loss = 370.69617, step = 8101 (0.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.769\n",
      "INFO:tensorflow:loss = 397.26843, step = 8201 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 616.53\n",
      "INFO:tensorflow:loss = 535.04126, step = 8301 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.146\n",
      "INFO:tensorflow:loss = 588.4576, step = 8401 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 587.25\n",
      "INFO:tensorflow:loss = 439.0677, step = 8501 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 590.07\n",
      "INFO:tensorflow:loss = 319.77496, step = 8601 (0.170 sec)\n",
      "INFO:tensorflow:global_step/sec: 543.836\n",
      "INFO:tensorflow:loss = 530.1021, step = 8701 (0.184 sec)\n",
      "INFO:tensorflow:global_step/sec: 595.454\n",
      "INFO:tensorflow:loss = 576.76025, step = 8801 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.267\n",
      "INFO:tensorflow:loss = 389.62415, step = 8901 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 578.83\n",
      "INFO:tensorflow:loss = 326.08893, step = 9001 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.928\n",
      "INFO:tensorflow:loss = 340.29456, step = 9101 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.049\n",
      "INFO:tensorflow:loss = 782.987, step = 9201 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.961\n",
      "INFO:tensorflow:loss = 412.34848, step = 9301 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 472.537\n",
      "INFO:tensorflow:loss = 530.09753, step = 9401 (0.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 599.128\n",
      "INFO:tensorflow:loss = 315.37436, step = 9501 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 574.641\n",
      "INFO:tensorflow:loss = 525.45, step = 9601 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 603.964\n",
      "INFO:tensorflow:loss = 434.50873, step = 9701 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 602.782\n",
      "INFO:tensorflow:loss = 548.75134, step = 9801 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 524.313\n",
      "INFO:tensorflow:loss = 665.2419, step = 9901 (0.191 sec)\n",
      "INFO:tensorflow:global_step/sec: 604.956\n",
      "INFO:tensorflow:loss = 667.2569, step = 10001 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.275\n",
      "INFO:tensorflow:loss = 372.31674, step = 10101 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 613.309\n",
      "INFO:tensorflow:loss = 388.23935, step = 10201 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.244\n",
      "INFO:tensorflow:loss = 430.8089, step = 10301 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 607.631\n",
      "INFO:tensorflow:loss = 313.81006, step = 10401 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 601.906\n",
      "INFO:tensorflow:loss = 434.82666, step = 10501 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 603.853\n",
      "INFO:tensorflow:loss = 584.3874, step = 10601 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.162\n",
      "INFO:tensorflow:loss = 751.5791, step = 10701 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 600.821\n",
      "INFO:tensorflow:loss = 461.9019, step = 10801 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 576.126\n",
      "INFO:tensorflow:loss = 802.6651, step = 10901 (0.174 sec)\n",
      "INFO:tensorflow:global_step/sec: 611.812\n",
      "INFO:tensorflow:loss = 388.02548, step = 11001 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 605.443\n",
      "INFO:tensorflow:loss = 294.756, step = 11101 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.753\n",
      "INFO:tensorflow:loss = 472.02106, step = 11201 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.727\n",
      "INFO:tensorflow:loss = 607.8582, step = 11301 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 594.672\n",
      "INFO:tensorflow:loss = 474.89624, step = 11401 (0.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.719\n",
      "INFO:tensorflow:loss = 664.8982, step = 11501 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 621.157\n",
      "INFO:tensorflow:loss = 420.11337, step = 11601 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 565.088\n",
      "INFO:tensorflow:loss = 839.2773, step = 11701 (0.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 546.696\n",
      "INFO:tensorflow:loss = 447.22757, step = 11801 (0.183 sec)\n",
      "INFO:tensorflow:global_step/sec: 608.709\n",
      "INFO:tensorflow:loss = 403.42572, step = 11901 (0.164 sec)\n",
      "INFO:tensorflow:global_step/sec: 603.021\n",
      "INFO:tensorflow:loss = 560.28796, step = 12001 (0.166 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.936\n",
      "INFO:tensorflow:loss = 451.91327, step = 12101 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 620.778\n",
      "INFO:tensorflow:loss = 462.38687, step = 12201 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 615.543\n",
      "INFO:tensorflow:loss = 519.8686, step = 12301 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 617.86\n",
      "INFO:tensorflow:loss = 343.48935, step = 12401 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.699\n",
      "INFO:tensorflow:loss = 459.7069, step = 12501 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 614.266\n",
      "INFO:tensorflow:loss = 451.0262, step = 12601 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.696\n",
      "INFO:tensorflow:loss = 644.85876, step = 12701 (0.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 618.436\n",
      "INFO:tensorflow:loss = 387.2374, step = 12801 (0.162 sec)\n",
      "INFO:tensorflow:global_step/sec: 510.743\n",
      "INFO:tensorflow:loss = 401.23904, step = 12901 (0.196 sec)\n",
      "INFO:tensorflow:global_step/sec: 612.738\n",
      "INFO:tensorflow:loss = 328.18866, step = 13001 (0.163 sec)\n",
      "INFO:tensorflow:global_step/sec: 606.633\n",
      "INFO:tensorflow:loss = 421.62338, step = 13101 (0.165 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.28\n",
      "INFO:tensorflow:loss = 452.11713, step = 13201 (0.167 sec)\n",
      "INFO:tensorflow:global_step/sec: 598.676\n",
      "INFO:tensorflow:loss = 422.7166, step = 13301 (0.167 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13334 into cylinders_trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 677.46967.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-03-14-13:59:55\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from cylinders_trained/model.ckpt-13334\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-03-14-13:59:55\n",
      "INFO:tensorflow:Saving dict for global step 13334: average_loss = 15.414234, global_step = 13334, loss = 30828.469, rmse = 3.9260964\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -rf cylinders_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/cylinder_prediction_module\n",
    "gcloud ml-engine local train \\\n",
    "    --module-name=trainer.task \\\n",
    "    --job-dir=cylinders_trained \\\n",
    "    --package-path=$(pwd)/trainer \\\n",
    "    -- \\\n",
    "    --batch_size=30 \\\n",
    "    --learning_rate=0.02 \\\n",
    "    --output_dir=cylinders_trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shut TensorBoard down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped TensorBoard with pid 7608\n",
      "Stopped TensorBoard with pid 7645\n"
     ]
    }
   ],
   "source": [
    "pids_df = TensorBoard.list()\n",
    "if not pids_df.empty:\n",
    "    for pid in pids_df['pid']:\n",
    "        TensorBoard().stop(pid)\n",
    "        print('Stopped TensorBoard with pid {}'.format(pid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hyperparam.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing hyperparam.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile hyperparam.yaml\n",
    "trainingInput:\n",
    "    hyperparameters:\n",
    "        goal: MINIMIZE\n",
    "        maxTrials: 16\n",
    "        maxParallelTrials: 2\n",
    "        hyperparameterMetricTag: rmse\n",
    "        params:\n",
    "        - parameterName: batch_size\n",
    "          type: INTEGER\n",
    "          minValue: 8\n",
    "          maxValue: 64\n",
    "          scaleType: UNIT_LINEAR_SCALE\n",
    "        - parameterName: learning_rate\n",
    "          type: DOUBLE\n",
    "          minValue: 0.01\n",
    "          maxValue: 0.1\n",
    "          scaleType: UNIT_LOG_SCALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit the hyperparameter tuning job to Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: cylinders_190314_141023\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing gs://cylinders/cylinders_trained/packages/7ecd38714b6a9e928ec1068e385f7e14c7de6db48d86fa031a728edc445797d0/trainer-0.0.0.tar.gz#1552572021789591...\n",
      "/ [1 objects]                                                                   \r\n",
      "Operation completed over 1 objects.                                              \n",
      "Job [cylinders_190314_141023] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe cylinders_190314_141023\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs cylinders_190314_141023\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://${BUCKET}/cylinders_trained   # CHANGE bucket name appropriately\n",
    "gsutil rm -rf $OUTDIR\n",
    "export PYTHONPATH=${PYTHONPATH}:${PWD}/cylinder_prediction_module\n",
    "gcloud ml-engine jobs submit training cylinders_$(date -u +%y%m%d_%H%M%S) \\\n",
    "    --config=hyperparam.yaml \\\n",
    "    --module-name=trainer.task \\\n",
    "    --package-path=$(pwd)/cylinder_prediction_module/trainer \\\n",
    "    --job-dir=$OUTDIR \\\n",
    "    --runtime-version=$TFVERSION \\\n",
    "    --\\\n",
    "    --output_dir=$OUTDIR \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monitor the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "createTime: '2019-03-14T14:10:26Z'\n",
      "etag: YgrWrjIYK_8=\n",
      "jobId: cylinders_190314_141023\n",
      "startTime: '2019-03-14T14:10:28Z'\n",
      "state: RUNNING\n",
      "trainingInput:\n",
      "  args:\n",
      "  - --output_dir=gs://cylinders/cylinders_trained\n",
      "  hyperparameters:\n",
      "    goal: MINIMIZE\n",
      "    hyperparameterMetricTag: rmse\n",
      "    maxParallelTrials: 2\n",
      "    maxTrials: 16\n",
      "    params:\n",
      "    - maxValue: 64.0\n",
      "      minValue: 8.0\n",
      "      parameterName: batch_size\n",
      "      scaleType: UNIT_LINEAR_SCALE\n",
      "      type: INTEGER\n",
      "    - maxValue: 0.1\n",
      "      minValue: 0.01\n",
      "      parameterName: learning_rate\n",
      "      scaleType: UNIT_LOG_SCALE\n",
      "      type: DOUBLE\n",
      "  jobDir: gs://cylinders/cylinders_trained\n",
      "  packageUris:\n",
      "  - gs://cylinders/cylinders_trained/packages/a7c1b89ca7bf4f1154a0bf147a640e93d0cb6dfcb6ce32daf10858b63358073b/trainer-0.0.0.tar.gz\n",
      "  pythonModule: trainer.task\n",
      "  region: europe-west1\n",
      "  runtimeVersion: '1.8'\n",
      "trainingOutput:\n",
      "  isHyperparameterTuningJob: true\n",
      "\n",
      "View job in the Cloud Console at:\n",
      "https://console.cloud.google.com/ml/jobs/cylinders_190314_141023?project=cylinders\n",
      "\n",
      "View logs at:\n",
      "https://console.cloud.google.com/logs?resource=ml.googleapis.com%2Fjob_id%2Fcylinders_190314_141023&project=cylinders\n"
     ]
    }
   ],
   "source": [
    "!gcloud ml-engine jobs describe cylinders_190314_141023  # Change jobId to what the previous cell printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
